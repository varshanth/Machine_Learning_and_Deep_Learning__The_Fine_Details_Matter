# Attention Mechanism

## Basics
* A visual introduction:  
https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3
* An exhaustive explanation of various attention mechanisms with mathematical formulation:  
https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html  
https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html
* Some basic code for understanding purposes:  
https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/  

## Transformer - Scaled Dot Product Attention
* For understanding:  
https://jalammar.github.io/illustrated-transformer/  
https://towardsdatascience.com/transformers-explained-visually-not-just-how-but-why-they-work-so-well-d840bd61a9d3  
https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0  
Search for Transformer: https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html

* Borealis AI Tutorials:  
  a. Intro: https://www.borealisai.com/en/blog/tutorial-14-transformers-i-introduction/  
  b. Extensions, Positional Encoding: https://www.borealisai.com/en/blog/tutorial-16-transformers-ii-extensions/  
  c. Overcoming the difficulty of training transformers: https://www.borealisai.com/en/blog/tutorial-17-transformers-iii-training/  

* Longformer - Analyzing long documents: https://towardsdatascience.com/longformer-the-long-document-transformer-cdfeefe81e89
